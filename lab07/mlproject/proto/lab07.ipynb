{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, StringIndexer, IndexToString}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder()\n",
    "    .appName(\"MLProject Lab07 DE\")\n",
    "    //.master(\"yarn\")\n",
    "    //.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "    .config(\"spark.driver.memory\", \"9g\")\n",
    "    .config(\"spark.driver.cores\", \"3\")\n",
    "    .config(\"spark.executor.instances\", \"6\")\n",
    "    .config(\"spark.executor.memory\", \"9g\")\n",
    "    .config(\"spark.executor.cores\", \"3\")\n",
    "    //.config(\"spark.sql.shuffle.partitions\", \"81\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val trainDir = spark.conf.get(\"spark.train.train_dir\")\n",
    "val trainDir = \"/labs/laba07/laba07.json\"\n",
    "//val modelPath = spark.conf.get(\"spark.train.pipeline_dir\")\n",
    "val modelPath = \"/user/andrey.blednykh2/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(Seq(\n",
    "    StructField(\"gender_age\", StringType, true),\n",
    "    StringField(\"uid\", StringType, true),\n",
    "    StringField(\"visits\", StringType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sampling ratio\n",
    "val jsonWeblogs = spark.read.json(trainDir)\n",
    "//    .select(from_json(col(\"value\"), schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWeblogs.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val training = jsonWeblogs\n",
    "    .withColumn(\"parsedVisits\", explode(col(\"visits\")))\n",
    "    .withColumn(\"timestamp\", col(\"parsedVisits.timestamp\"))\n",
    "    .withColumn(\"urlRaw\", col(\"parsedVisits.url\"))\n",
    "    //.withColumn(\"host\", lower(callUDF(\"parse_url\", $\"urlRaw\", lit(\"HOST\")))) // a log of bags... http, https, NULL domains... for partial correct url\n",
    "    .withColumn(\"cleaning1\", regexp_replace(col(\"urlRaw\"), \"https://\", \"http://\"))\n",
    "    .withColumn(\"cleaning2\", regexp_replace(col(\"cleaning1\"), \"http://http://\", \"http://\")) \n",
    "    .withColumn(\"host\", regexp_extract($\"cleaning2\",\"^(([^:\\\\/?#]+):)?(\\\\/\\\\/([^\\\\/?#]*))?([^?#]*)(\\\\?([^#]*))?(#(.*))?\", 4))\n",
    "    .withColumn(\"cleaning3\", regexp_replace($\"host\", \"^www.\", \"\"))\n",
    "    .withColumn(\"domain\", regexp_replace($\"cleaning3\", \"^\\\\.\", \"\"))  // special for kasparov with www1.\n",
    "    .select(\"uid\", \"gender_age\", \"domain\")\n",
    "    .groupBy(\"uid\", \"gender_age\")\n",
    "    .agg(collect_list(\"domain\").alias(\"domains\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainParsed.show(2, 100, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainParsed.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cv = new CountVectorizer()\n",
    "    .setInputCol(\"domains\")\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"gender_age\")\n",
    "    .setOutputCol(\"label\")\n",
    "    .fit(training); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(1000)\n",
    "    .setRegParam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lc = new IndexToString()\n",
    "    .setInputCol(\"prediction\")\n",
    "    .setOutputCol(\"predictedLabel\")\n",
    "    .setLabels(indexer.labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(cv, indexer, lr, lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kafkaInputParams = Map(\n",
    "    \"kafka.bootstrap.servers\" -> \"spark-master-1:6667\",\n",
    "    \"subscribe\" -> \"andrey.blednykh2\",\n",
    "    \"startingOffsets\" -> \"\"\"earliest\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kafkaWeblogs = spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaInputParams)\n",
    "    .load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(Seq(\n",
    "    StringField(\"uid\", StringType, true),\n",
    "    StringField(\"visits\", StringType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schemaTest = ArrayType(\n",
    "    StructType(Seq(\n",
    "        StructField(\"url\", StringType, true),\n",
    "        StructField(\"timestamp\", StringType, true)\n",
    "      ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kafkaValues = kafkaData\n",
    "    .select(col(\"value\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kafkaWeblogs = kafkaValues\n",
    "    .withColumn(\"value\", from_json(col(\"value\"), schema))\n",
    "    .select(col(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testing = kafkaWeblogs\n",
    "    .withColumn(\"visitsAR\", from_json(col(\"visits\"), schemaTest))\n",
    "    .withColumn(\"parsedVisits\", explode(col(\"visitsAR\")))\n",
    "    .withColumn(\"timestamp\", col(\"parsedVisits.timestamp\"))\n",
    "    .withColumn(\"urlRaw\", col(\"parsedVisits.url\"))\n",
    "    //.withColumn(\"host\", lower(callUDF(\"parse_url\", $\"urlRaw\", lit(\"HOST\")))) // a log of bags... http, https, NULL domains... for partial correct url\n",
    "    .withColumn(\"cleaning1\", regexp_replace(col(\"urlRaw\"), \"https://\", \"http://\"))\n",
    "    .withColumn(\"cleaning2\", regexp_replace(col(\"cleaning1\"), \"http://http://\", \"http://\"))\n",
    "    .withColumn(\"host\", regexp_extract($\"cleaning2\",\"^(([^:\\\\/?#]+):)?(\\\\/\\\\/([^\\\\/?#]*))?([^?#]*)(\\\\?([^#]*))?(#(.*))?\", 4))\n",
    "    .withColumn(\"cleaning3\", regexp_replace($\"host\", \"^www.\", \"\"))\n",
    "    .withColumn(\"domain\", regexp_replace($\"cleaning3\", \"^\\\\.\", \"\"))  // special for kasparov with www1.\n",
    "    .select(\"uid\", \"domain\")\n",
    "    .groupBy(\"uid\")\n",
    "    .agg(collect_list(\"domain\").alias(\"domains\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model = PipelineModel.load(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val logPrediction = model.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultForKafka = logPrediction\n",
    "    .select(col(\"uid\"), col(\"predictedLabel\").alias(\"gender_age\")) //.show(20, 200, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kafkaOutputParams = Map(\n",
    "    \"kafka.bootstrap.servers\" -> \"spark-master-1:6667\",\n",
    "    \"topic\" -> \"andrey_blednykh2_lab07_out\",\n",
    "    \"checkpointLocation\" -> \"streaming/checkpoint/\",\n",
    "    \"truncate\" -> \"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultForKafka\n",
    "    .select(col(\"uid\").cast(\"string\").alias(\"key\"),\n",
    "        to_json(struct(\"uid\", \"gender_age\")).alias(\"value\"))\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaOutputParams)\n",
    "    .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession\n",
    "    .active\n",
    "    .streams\n",
    "    .active\n",
    "    .foreach { x =>\n",
    "        val desc = x.lastProgress.sources.head.description\n",
    "        x.stop\n",
    "        println(s\"Stopped ${desc}\")\n",
    "    }  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
