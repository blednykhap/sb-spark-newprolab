{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val itemsUpdate = 1\n",
    "val outputDir = \"/user/andrey.blednykh2/users-items\"\n",
    "val inputDir = \"/user/andrey.blednykh2/visits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder()\n",
    "    .appName(\"UsersItems Lab05 DE\")\n",
    "    //.master(\"yarn\")\n",
    "    //.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.driver.cores\", \"3\")\n",
    "    .config(\"spark.executor.instances\", \"8\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.executor.cores\", \"3\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val events = spark.read.json(s\"$inputDir/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.show(2,200,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val new_max_date = events\n",
    "    .withColumn(\"timestamp\", date_format((col(\"timestamp\") / 1000).cast(TimestampType), \"yyyyMMdd\"))\n",
    "    .agg(max(col(\"timestamp\").cast(\"integer\")))\n",
    "    .take(1)(0).getInt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_max_date(path: String) : String = {\n",
    "    \n",
    "    val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "    \n",
    "    try {\n",
    "        val dirs = fs.listStatus(new Path(path))\n",
    "            .filter(_.isDirectory)\n",
    "            .map(_.getPath.getName.toInt)\n",
    "\n",
    "        if (dirs.size >= 1) {\n",
    "            dirs.reduceLeft(_ max _).toString\n",
    "        } else {\n",
    "            \"\"\n",
    "        }               \n",
    "    } catch { case _: Throwable => \"\"} \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val old_max_date = get_old_max_date(s\"$outputDir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val new_users_items = events\n",
    "    .na.drop(Seq(\"uid\"))\n",
    "    .withColumn(\"norm_column\", regexp_replace(lower(col(\"item_id\")), \"[-| ]\", \"_\"))\n",
    "    .withColumn(\"new_column\", \n",
    "                when(col(\"event_type\") === \"buy\", concat(lit(\"buy_\"), col(\"norm_column\")))\n",
    "                    .otherwise(concat(lit(\"view_\"), col(\"norm_column\"))))\n",
    "    .groupBy(col(\"uid\"))\n",
    "    .pivot(col(\"new_column\"))\n",
    "    .agg(count(lit(1)))\n",
    "    .na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_users_items.show(2,200,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((itemsUpdate == 1) && (old_max_date != \"\") && (new_max_date > old_max_date.toInt)) {\n",
    "    val old_users_items = spark.read.parquet(s\"$outputDir/$old_max_date\")\n",
    "    \n",
    "    val users_items = new_users_items\n",
    "        .union(old_users_items)\n",
    "        .groupBy(col(\"uid\"))\n",
    "        .sum(new_users_items.columns.drop(1): _*)\n",
    "    \n",
    "    users_items\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(s\"$outputDir/$new_max_date\")  \n",
    "} else {\n",
    "    new_users_items\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(s\"$outputDir/$new_max_date\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
